{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "published-traffic",
   "metadata": {},
   "source": [
    "# Matrices Revisited\n",
    "\n",
    "toc: true\n",
    "badges: false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca221b91",
   "metadata": {},
   "source": [
    "## Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf6889",
   "metadata": {},
   "source": [
    "Norm is a function which when applied, gives a measure of distance-ness from the origin, or the size of the **vector**. This function has the following properties\n",
    "\n",
    "$$f(x) = 0 \\text{ then } x = 0 \\tag{1}$$\n",
    "\n",
    "$$f(x+y) \\leq f(x) + f(y) \\cdots \\text{ Triangle Inequality} \\tag{2}$$\n",
    "\n",
    "$$f(\\alpha x) = |\\alpha| f(x) \\text{  } \\forall \\alpha \\in \\mathbb{R} \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe99ebf",
   "metadata": {},
   "source": [
    "Examples of Norm are,\n",
    "\n",
    "$$||x||_p = \\left(\\sum_i |x_i|^p\\right)^\\frac{1}{p} \\tag{$L^p$ Norm}$$\n",
    "\n",
    "When $p = 2$, we have $L^2$ norm, also known as Euclidean norm. For practical purposes we use squared $L^2$ norm.\n",
    "$$||x||_p = \\sum_i |x_i|^2$$\n",
    "\n",
    "However, when when we are working with values near $0$, a change of $\\epsilon$ will cause the squared $L^2$ norm to change by $\\epsilon^2$, which will be very small. Thus we can use $L^1$ norm in those cases.\n",
    "\n",
    "$$||x||_1 = \\sum_i |x_i| \\tag{$L^1$ Norm}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-trace",
   "metadata": {},
   "source": [
    "## Orthogonal matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-automation",
   "metadata": {},
   "source": [
    "Properties of orthogonal matrices\n",
    "- They must be square\n",
    "- Rows are mutually orthonormal\n",
    "- Columns are mutually orthonormal\n",
    "\n",
    "> Two vectors $x$ and $y$ are **orthonormal** if their dot product is 0 and they have unit norm.\n",
    "\n",
    "As a result of the definition, the below property follows for any orthogonal matrix $A$,\n",
    "\n",
    "$$A^T A = A A^T = I$$\n",
    "\n",
    "$$A^{-1} = A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-promise",
   "metadata": {},
   "source": [
    "### Multiplying with orthogonal matrices leads to rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-opera",
   "metadata": {},
   "source": [
    "To prove that multiplying with an orthogonal matrix leads to a rotation, we need to prove two things,\n",
    "- The length/norm of the vector is preserved after matrix operation.\n",
    "- The angles between vectors are preserved after matrix operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-caribbean",
   "metadata": {},
   "source": [
    "#### Length is preserved\n",
    "\n",
    "Let $c$ be an orthogonal matrix, then the squared norm of the transformed vector is as follows,\n",
    "$$||c \\vec{x}||^2$$\n",
    "\n",
    "$$= c \\vec{x} \\cdot c \\vec{x}$$\n",
    "\n",
    "$$= (c \\vec{x})^T c \\vec{x}$$\n",
    "\n",
    "$$= \\vec{x}^T c^T c \\vec{x}$$\n",
    "\n",
    "$$= \\vec{x}^T I \\vec{x}$$\n",
    "\n",
    "$$= \\vec{x}^T \\vec{x}$$\n",
    "\n",
    "$$= \\vec{x} \\cdot \\vec{x}$$\n",
    "\n",
    "$$= || \\vec{x} ||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-tongue",
   "metadata": {},
   "source": [
    "#### Angle between vectors is preserved\n",
    "\n",
    "Let $\\vec{v}$ and $\\vec{w}$ be two vectors and let $\\theta$ be the angle between them.\n",
    "\n",
    "$$\\therefore \\text{ } \\cos \\theta = \\frac{\\vec{v} \\cdot \\vec{w}}{||\\vec{v}||\\text{ }||\\vec{w}||}$$\n",
    "\n",
    "Let $\\theta_c$ be the angle between the rotated vectors.\n",
    "\n",
    "$$\\therefore \\text{ } \\cos \\theta_c = \\frac{C\\vec{v} \\cdot C\\vec{w}}{||C\\vec{v}||\\text{ }||C\\vec{w}||}$$\n",
    "\n",
    "$$ = \\frac{(C\\vec{w})^T C\\vec{v}}{||\\vec{v}||\\text{ }||\\vec{w}||}$$\n",
    "\n",
    "$$ = \\frac{\\vec{w}^T C^T C\\vec{v}}{||\\vec{v}||\\text{ }||\\vec{w}||}$$\n",
    "\n",
    "$$ = \\frac{\\vec{w}^T I \\vec{v}}{||\\vec{v}||\\text{ }||\\vec{w}||}$$\n",
    "\n",
    "$$ = \\frac{\\vec{w}^T \\vec{v}}{||\\vec{v}||\\text{ }||\\vec{w}||}$$\n",
    "\n",
    "$$ = \\frac{\\vec{v} \\cdot \\vec{w}}{||\\vec{v}||\\text{ }||\\vec{w}||}$$\n",
    "\n",
    "$$ \\cos \\theta_c = \\cos \\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-northeast",
   "metadata": {},
   "source": [
    "## Eigen vectors and Eigen values\n",
    "\n",
    "For a matrix $A$, a vector $x$ and some scalar constant $\\lambda$, we can have an equation as below,\n",
    "\n",
    "$$A x = \\lambda x$$\n",
    "\n",
    "Here, $x$ becomes the eigen vector and $\\lambda$ its corresponding eigen value.\n",
    "\n",
    "To get an intuition for this equation, we can start by thinking of a matrix multiplication on a vector as application of a function that leads to a transformation in the space. This space in which the transformation take place will have few axes (imaging $x$, $y$ and $z$, however more than three are possible). The transformation by the matrix $A$ can be decomposed into sub-transformations along these axes. That scaling factor $\\lambda$ is called the eigen value.\n",
    "\n",
    "Let us assume a vector $\\vec{v}$ along the axes $x$ ($x$ from the equation above). Now, the dot product of $\\vec{v}$ will cancel out with all the other axes except $x$. Thus the scaling (transformation) will happen only in the direction of $x$ axes.\n",
    "\n",
    "> \"Cancels out\" is a crude term which holds only for matrices whose eigen vectors are orthogonal (see below). Let us assume that the transformation is quantified by an array with each value as a metric of change in the axes other than $x$, then we have chosen the case for other axes as say $[0, 0, 0, 0, 0]$, which could well be $[-1, 2, -3, 4, -2]$ where sum of both the arrays is 0, but the former is easier to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-victoria",
   "metadata": {},
   "source": [
    "### Eigen vectors of a symmetric matrix are orthogonal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-perth",
   "metadata": {},
   "source": [
    "Let $A$ be a symmetric matrix with $x$ and $y$ as two of its eigen vectors and $\\lambda_1$ and $\\lambda_2$ be distinct and corresponding eigen values.\n",
    "\n",
    "$$\\therefore \\text{ } A x = \\lambda_1 x  \\tag{1}$$\n",
    "Multiplying both sides on left with $y^T$\n",
    "$$\\therefore \\text{ } y^T A x = \\lambda_1 y^T x$$\n",
    "Taking transpose on both sides,\n",
    "$$\\therefore \\text{ } \\left(y^T A x\\right)^T = \\lambda_1 \\left(y^T x\\right)^T$$\n",
    "\n",
    "$$\\therefore \\text{ } x^T A^T y = \\lambda_1 x^T y$$\n",
    "Since $A$ is symmetric, we have $A = A^T$\n",
    "$$\\therefore \\text{ } x^T A y = \\lambda_1 x^T y \\tag{2}$$\n",
    "\n",
    "Also,\n",
    "$$\\therefore \\text{ } A y = \\lambda_2 y  \\tag{3}$$\n",
    "Multiplying both sides on left with $x^T$\n",
    "$$\\therefore \\text{ } x^T A y = \\lambda_2 x^T y \\tag{4}$$\n",
    "Subtracting $(4)$ from $(2)$ we get,\n",
    "$$x^T y \\cdot (\\lambda_1 - \\lambda_2) = 0$$\n",
    "Since $\\lambda_1 \\neq \\lambda_2$ , we have,\n",
    "$$x^T y = 0$$\n",
    "\n",
    "This implies that $y \\cdot x = 0$ or that $x$ and $y$ are orthogonal vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-being",
   "metadata": {},
   "source": [
    "## Eigen Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-papua",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-establishment",
   "metadata": {},
   "source": [
    "## Spectral norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
