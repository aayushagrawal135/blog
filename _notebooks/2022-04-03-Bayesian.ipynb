{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34dc783",
   "metadata": {},
   "source": [
    "# Bayesian Approach\n",
    "\n",
    "> I had enrolled in the [Machine Learning](https://rajeshhr.github.io/ml-2022/) course in my Masters. Following is my attempt to document a lecture as I understand about all things Bayesian.\n",
    "\n",
    "Bayesian approaches basically assumes a prior probability distribution over the random variables to quantify the uncertainty in their values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761fcbf",
   "metadata": {},
   "source": [
    "## Key Ideas\n",
    "\n",
    "- **Prior: $p(\\theta)$**\n",
    "\n",
    "    $\\theta$ is representative of all the paramaters in the model. This prior distribution signifies the state of the parameters before the training begun.\n",
    "\n",
    "\n",
    "- **Likelihood: $p(y | \\theta, x)$**\n",
    "\n",
    "    $y$ is an output for the corresponding given input $x$. This phase is similar to training the model where we are predicting the $y$, when we have the train data $x$ and the prior $\\theta$ parameters. \n",
    "\n",
    "\n",
    "- **Posterior: $p(\\theta | x, y)$**\n",
    "\n",
    "    Once the training is complete, the parameters of the model must have been tuned, given the pair of train data $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5b67b",
   "metadata": {},
   "source": [
    "## Posterior is proportional to joint\n",
    "\n",
    "<!-- $$\n",
    "\\begin{aligned}\n",
    "p(\\theta \\mid x, y) &= \\frac{p(\\theta, x, y)}{p(x, y)} \\\\\n",
    "&= \\frac{p(x) \\text{ } p(\\theta, x, y)}{p(x) \\text{ } p(x, y)} \\\\\n",
    "&=  \\frac{p(\\theta, y \\mid x)}{p(y \\mid x)} \\\\\n",
    "& \\propto p(\\theta, y \\mid x) \\\\\n",
    "\\end{aligned}\n",
    "$$ -->\n",
    "\n",
    "\n",
    "So, $p(y \\mid x)$ acts like a normalising contant here. Dividing by it does not change the inherent shape of the function, just brings the area under the curve to $1$ so that we can call it probability.\n",
    "\n",
    "Then, $p(\\theta, y \\mid x)$ is the joint distribution here since initially we were given $x$ and, $\\theta$ and $y$ were not given. So we take joint of not given over given variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45931849",
   "metadata": {},
   "source": [
    "## Posterior = Prior * Likelihood\n",
    "\n",
    "This is a very frequently used equality whenever Bayesian approaches are involved. Now that we know what the individual terms are, we can verify (if) it is true using simpler building block Bayes rule\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\theta, y \\mid x) &= \\frac{p(\\theta, x, y)}{p(x)} \\\\\n",
    "&= p(\\theta)  \\frac{p(\\theta, x, y)}{p(\\theta) \\text{ } p(x)} \\\\\n",
    "&= p(\\theta) \\text{ } p(y \\mid \\theta, x) \\frac{p(\\theta, x)}{p(\\theta) \\text{ } p(x)} \\\\\n",
    "&= p(\\theta) \\text{ } p(y \\mid \\theta, x) \\tag{$\\theta$ and $x$ assumed independent}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, the equality is true only when $p(\\theta, x) = p(\\theta) \\text{ } p(x)$, that is, $x$ and $\\theta$ are independent. It may sound intuitive that the input data $x$ has nothing to do with the parameters of the model $\\theta$ before the model training begins. However, writing the above steps just makes it explicit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81326c74",
   "metadata": {},
   "source": [
    "## Predicting a new point $x^{\\star}$, given x, y\n",
    "\n",
    "Let the prediction for the new point $x^{\\star}$ be $y^{\\star}$, therefore, we want to find the distribution $p(y^{\\star} \\mid x^{\\star}, x, y)$.\n",
    "\n",
    "Marginalizing over posterior $p(\\theta \\mid x, y)$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y^{\\star} \\mid x^{\\star}, x, y) &= \\int p(y^{\\star} \\mid x^{\\star}, \\theta) \\text{ } p(\\theta \\mid x, y) d\\theta \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> I find it easier to quickly use it as a kind of chain rule. In $p(\\theta \\mid x, y)$, $x$ and $y$ were given to us and we found out $\\theta$. Next, we use this $\\theta$ and the remaining $x^{\\star}$ in the given side of the $\\mid$ (bar), thus we can write $p(y^{\\star} \\mid x^{\\star}, \\theta)$.\n",
    "\n",
    "When marginalizing over posterior $p(\\theta \\mid x, y)$, we are going over (weighted average) all the models (combination of paramters $\\theta$) for given $x$ and $y$. We are using these models as priors for $x^{\\star}$ and $y^{\\star}$ to find their likelihood $p(y^{\\star} \\mid x^{\\star}, \\theta)$\n",
    "\n",
    "Here we assume that $(x^{\\star}, y^{\\star})$ are from the same distribution as $(x, y)$ because we have covered only those models that were derived from $(x, y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e9936",
   "metadata": {},
   "source": [
    "Below is an alternative way to do the same thing, however it gives clearer idea about the assumptions we make.\n",
    "\n",
    "<!-- $$\n",
    "\\begin{aligned}\n",
    "p(\\theta, y \\mid x) &= \\frac{p(\\theta, x, y)}{p(x)} \\\\\n",
    "&= p(\\theta)  \\frac{p(\\theta, x, y)}{p(\\theta) \\text{ } p(x)} \\\\\n",
    "&= p(\\theta) \\text{ } p(y \\mid \\theta, x) \\frac{p(\\theta, x)}{p(\\theta) \\text{ } p(x)} \\\\\n",
    "&= p(\\theta) \\text{ } p(y \\mid \\theta, x) \\tag{$\\theta$ and $x$ assumed independent}\\\\\n",
    "\\end{aligned}\n",
    "$$ -->\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y^{\\star} \\mid x^{\\star}, x, y) &= \\int p(y^{\\star}, \\theta \\mid x^{\\star}, x, y) d\\theta \\tag{Marginalizing over $\\theta$}\\\\\n",
    "&= \\int p(y^{\\star} \\mid x^{\\star}, x, y, \\theta)p(\\theta \\mid x^{\\star}, x, y) d\\theta \\tag{$p(A \\mid B, C) \\text{ } p(B \\mid C) = p(A, B \\mid C)$}\\\\\n",
    "&= \\int \\frac{p(y^{\\star}, x, y \\mid x^{\\star}, \\theta)}{p(x, y)}  \\frac{p(\\theta, x^{\\star} \\mid x, y)}{p(x^{\\star})} d\\theta \\\\\n",
    "&= \\int \\left[\\frac{p(x, y \\mid x^{\\star}, y^{\\star}, \\theta)}{p(x, y)} \\frac{p(x^{\\star} \\mid \\theta, x, y)}{p(x^{\\star})}\\right] \\cdot \\left[p(y^{\\star} \\mid x^{\\star}, \\theta) p(\\theta \\mid x, y)\\right] d\\theta \\\\ \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76186a9",
   "metadata": {},
   "source": [
    "Therefore, this would be equal to the original result when the first big bracket is $1$. Thus $p(x, y \\mid x^{\\star}, y^{\\star}, \\theta) = p(x, y)$ and $p(x^{\\star} \\mid \\theta, x, y) = p(x^{\\star})$ must be true. Intuitively it means that $(x^{\\star}, y^{\\star})$ come from the same distribution as $(x, y)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
