---
keywords: fastai
description: I had enrolled in the <a href='https://rajeshhr.github.io/ml-2022/'>Machine Learning</a> course in my Masters. Following is my attempt to document a lecture as I understand about all things Bayesian.
title: Bayesian Approach
nb_path: _notebooks/2022-04-03-Bayesian.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-04-03-Bayesian.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Key-Ideas">Key Ideas<a class="anchor-link" href="#Key-Ideas"> </a></h2><ul>
<li><p><strong>Prior: $p(\theta)$</strong></p>
<p>$\theta$ is representative of all the paramaters in the model. This prior distribution signifies the state of the parameters before the training begun.</p>
</li>
</ul>
<ul>
<li><p><strong>Likelihood: $p(y | \theta, x)$</strong></p>
<p>$y$ is an output for the corresponding given input $x$. This phase is similar to training the model where we are predicting the $y$, when we have the train data $x$ and the prior $\theta$ parameters.</p>
</li>
</ul>
<ul>
<li><p><strong>Posterior: $p(\theta | x, y)$</strong></p>
<p>Once the training is complete, the parameters of the model must have been tuned, given the pair of train data $x$ and $y$.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Posterior-is-proportional-to-joint">Posterior is proportional to joint<a class="anchor-link" href="#Posterior-is-proportional-to-joint"> </a></h2><!-- $$
\begin{aligned}
p(\theta \mid x, y) &= \frac{p(\theta, x, y)}{p(x, y)} \\
&= \frac{p(x) \text{ } p(\theta, x, y)}{p(x) \text{ } p(x, y)} \\
&=  \frac{p(\theta, y \mid x)}{p(y \mid x)} \\
& \propto p(\theta, y \mid x) \\
\end{aligned}
$$ -->


<p>So, $p(y \mid x)$ acts like a normalising contant here. Dividing by it does not change the inherent shape of the function, just brings the area under the curve to $1$ so that we can call it probability.</p>
<p>Then, $p(\theta, y \mid x)$ is the joint distribution here since initially we were given $x$ and, $\theta$ and $y$ were not given. So we take joint of not given over given variables.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Posterior-=-Prior-*-Likelihood">Posterior = Prior * Likelihood<a class="anchor-link" href="#Posterior-=-Prior-*-Likelihood"> </a></h2><p>This is a very frequently used equality whenever Bayesian approaches are involved. Now that we know what the individual terms are, we can verify (if) it is true using simpler building block Bayes rule</p>
$$
\begin{aligned}
p(\theta, y \mid x) &amp;= \frac{p(\theta, x, y)}{p(x)} \\
&amp;= p(\theta)  \frac{p(\theta, x, y)}{p(\theta) \text{ } p(x)} \\
&amp;= p(\theta) \text{ } p(y \mid \theta, x) \frac{p(\theta, x)}{p(\theta) \text{ } p(x)} \\
&amp;= p(\theta) \text{ } p(y \mid \theta, x) \tag{$\theta$ and $x$ assumed independent}\\
\end{aligned}
$$<p>Therefore, the equality is true only when $p(\theta, x) = p(\theta) \text{ } p(x)$, that is, $x$ and $\theta$ are independent. It may sound intuitive that the input data $x$ has nothing to do with the parameters of the model $\theta$ before the model training begins. However, writing the above steps just makes it explicit.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Predicting-a-new-point-$x^{\star}$,-given-x,-y">Predicting a new point $x^{\star}$, given x, y<a class="anchor-link" href="#Predicting-a-new-point-$x^{\star}$,-given-x,-y"> </a></h2><p>Let the prediction for the new point $x^{\star}$ be $y^{\star}$, therefore, we want to find the distribution $p(y^{\star} \mid x^{\star}, x, y)$.</p>
<p>Marginalizing over posterior $p(\theta \mid x, y)$,
$$
\begin{aligned}
p(y^{\star} \mid x^{\star}, x, y) &amp;= \int p(y^{\star} \mid x^{\star}, \theta) \text{ } p(\theta \mid x, y) d\theta \\
\end{aligned}
$$</p>
<blockquote><p>I find it easier to quickly use it as a kind of chain rule. In $p(\theta \mid x, y)$, $x$ and $y$ were given to us and we found out $\theta$. Next, we use this $\theta$ and the remaining $x^{\star}$ in the given side of the $\mid$ (bar), thus we can write $p(y^{\star} \mid x^{\star}, \theta)$.</p>
</blockquote>
<p>When marginalizing over posterior $p(\theta \mid x, y)$, we are going over (weighted average) all the models (combination of paramters $\theta$) for given $x$ and $y$. We are using these models as priors for $x^{\star}$ and $y^{\star}$ to find their likelihood $p(y^{\star} \mid x^{\star}, \theta)$</p>
<p>Here we assume that $(x^{\star}, y^{\star})$ are from the same distribution as $(x, y)$ because we have covered only those models that were derived from $(x, y)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below is an alternative way to do the same thing, however it gives clearer idea about the assumptions we make.</p>
$$
\begin{aligned}
p(y^{\star} \mid x^{\star}, x, y) &amp;= \int p(y^{\star}, \theta \mid x^{\star}, x, y) d\theta \tag{Marginalizing over $\theta$} \\
\end{aligned}
$$<p>$ \because p(A \mid B, C) p(B \mid C) = p(A, B \mid C) $</p>
$$
\begin{aligned}
&amp;= \int p(y^{\star} \mid x^{\star}, x, y, \theta)p(\theta \mid x^{\star}, x, y) d\theta \\
&amp;= \int \frac{p(y^{\star}, x, y \mid x^{\star}, \theta)}{p(x, y)}  \frac{p(\theta, x^{\star} \mid x, y)}{p(x^{\star})} d\theta \\
&amp;= \int \left[\frac{p(x, y \mid x^{\star}, y^{\star}, \theta)}{p(x, y)} \frac{p(x^{\star} \mid \theta, x, y)}{p(x^{\star})}\right] \cdot \left[p(y^{\star} \mid x^{\star}, \theta) p(\theta \mid x, y)\right] d\theta \\ 
\end{aligned}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Therefore, this would be equal to the original result when the first big bracket is $1$. Thus $p(x, y \mid x^{\star}, y^{\star}, \theta) = p(x, y)$ and $p(x^{\star} \mid \theta, x, y) = p(x^{\star})$ must be true. Intuitively it means that $(x^{\star}, y^{\star})$ come from the same distribution as $(x, y)$.</p>

</div>
</div>
</div>
</div>
 

